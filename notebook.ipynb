{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live-stock detection (DeepForest)\n",
    "## Context\n",
    "### Purpose\n",
    "Implement and fine-tune a prebuilt Deep Learning model to detect livestock in airborne imagery. The model is implemented using PyTorch Lightning, which simplifies the training process and allows for easy checkpointing, enabling collaborative work by saving and sharing model progress.\n",
    "\n",
    "### Modelling Approach\n",
    "The [live-stock detection model](https://huggingface.co/weecology/deepforest-livestock) from the latest version (v1.4.0) of the [DeepForest](https://deepforest.readthedocs.io/en/latest/) Deep Learning model is used to predict bounding boxes corresponding to cattle from airborn RGB images.\n",
    "\n",
    "As a PyTorch Lightning module, this model can be fine-tuned easily, and checkpoints can be saved as [.safetensors](https://huggingface.co/docs/safetensors/index), making it possible to upload the trained models to platforms like Hugging Face for open collaboration.\n",
    "\n",
    "The prebuilt model was trained on a [limited dataset](https://new.wildlabs.net/discussion/global-model-livestock-detection-airborne-imagery-data-applications-and-needs). According to the package's documentation, \"the prebuilt models will always be improved by adding data from the target area\". As such, this notebook will explore the improvement in the model's performance in live-stock detection from fine-tuning on local data.\n",
    "\n",
    "### Description\n",
    "This notebook will explore the capabilities of the DeepForest package. In particular, it will demonstrate how to:\n",
    "\n",
    "- Detect livestock in airborne imagery using the prebuilt livestock detection model.\n",
    "- Fine-tune the model using a novel publicly-available dataset.\n",
    "- Evaluate the model's performance before and after fine-tuning.\n",
    "- Save and share model checkpoints throughout the process, allowing for reproducibility and collaboration on Hugging Face.\n",
    "\n",
    "### Highlights\n",
    "The prebuilt model was trained on 2585 training and 808 validation annotations, and its performance metrics on the test set (subset of images excluded from training/validation sets) showed substantial gains:\n",
    "\n",
    "- Box Recall: Improved from 0.4405 to 0.9535.\n",
    "- Box Precision: Improved from 0.5826 to 0.8587.\n",
    "- Mean IoU: Improved from 0.3135 to 0.6571.\n",
    "\n",
    "These results demonstrate the potential of improving the model further by sharing checkpoints as open-source models on Hugging Face, enabling collaborative enhancements based on additional local data.\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "* Cameron Appel (author), Queen Mary University of London, @camappel\n",
    "\n",
    "#### Modelling codebase\n",
    "* Ben Weinstein (maintainer & developer), University of Florida, @bw4sz\n",
    "* Henry Senyondo (support maintainer), University of Florida, @henrykironde\n",
    "* Ethan White (PI and author), University of Florida, @weecology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install deepforest==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import intake\n",
    "import xmltodict\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from deepforest import main\n",
    "from deepforest.visualize import plot_predictions\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "\n",
    "from shapely.geometry import box\n",
    "from skimage.exposure import equalize_hist\n",
    "\n",
    "import pooch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set project structure\n",
    "The cell below creates a separate folder to save the notebook outputs. This facilitates the reader to inspect inputs/outputs stored within a defined destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "notebook_folder = './notebook'\n",
    "if not os.path.exists(notebook_folder):\n",
    "    os.makedirs(notebook_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = os.path.join(notebook_folder, 'test_data')\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch RGB images from Zenodo\n",
    "\n",
    "Fetch sample images and ground-truth labels from [Zenodo](https://zenodo.org/records/13851270).\n",
    "\n",
    "Data were sourced from Harvard's publicly accessible [ODjAR Dataverse](https://dataverse.harvard.edu/dataverse/ODjAR). Specifically, G.J. Franke; Sander Mucher, 2021, \"Annotated cows in aerial images for use in deep learning models\", which includes \"a large dataset containing aerial images from fields in Juchowo, Poland and Wageningen, the Netherlands, with annotated cows present in the images using Pascal VOC XML Annotation Format.\"\n",
    "\n",
    "Given that this dataset is stored as a multi-part archive, it was necessary to download and unzip the files separately using `pyunpack`, then distribute the test subset on Zenodo.\n",
    "\n",
    "- Images are `.jpg` files\n",
    "- Ground-truth labels are represented by `test.csv` (image_path, xmin, ymin, xmax, ymax, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipped_files = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13851270/test_data.zip\",\n",
    "    known_hash=\"6a0a5b48fc9326e97c3cd8bdcabc2bcd131f3755f6ceabbf6976aefbfc87fb00\",\n",
    "    processor=pooch.Unzip(extract_dir=extract_dir),\n",
    "    path=f\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV (annotations)\n",
    "test_path = [file for file in unzipped_files if file.endswith('test.csv')][0]\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the baseline livestock detection model from Hugging Face using the load_model function provided by DeepForest. This function allows us to load pretrained models, including custom revisions, directly from Hugging Face.\n",
    "\n",
    "The model weights are saved in the [.safetensors](https://huggingface.co/docs/safetensors/index) format. Safetensors is a relatively new format, introduced in late 2022 by Hugging Face. It was developed to address security and performance concerns when loading large models, specifically avoiding the risk of arbitrary code execution that can occur with traditional formats like PyTorch’s .pt or TensorFlow’s .h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = main.deepforest()\n",
    "model.load_model(model_name=\"weecology/deepforest-livestock\", revision=\"main\")\n",
    "\n",
    "# Other pre-trained models:\n",
    "    # - weecology/deepforest-bird\n",
    "    # - weecology/deepforest-livestock\n",
    "    # - weecology/everglades-nest-detection\n",
    "    # - weecology/cropmodel-deadtrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate baseline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline performance of the pretrained livestock detection model is evaluated on the test set.\n",
    "\n",
    "Using the evaluate method, the model's performance is assessed by calculating key metrics such as __Box Recall__, __Box Precision__, and __Mean IoU__ (Intersection over Union), which indicate how accurately the model detects livestock in the test imagery. This serves as a benchmark to compare improvements after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label_dict = {'cow': 0}  # label defaults to 'tree'; change to 'cow', 'bird', etc.\n",
    "model.config['gpus'] = '-1'  # Use GPU (set to '0' for the first GPU or '-1' for all GPUs)\n",
    "model.config['workers'] = 0\n",
    "\n",
    "# Set the directory to save the results of the pretrained model\n",
    "baseline_save_dir = os.path.join(notebook_folder, 'baseline_pred_result')\n",
    "os.makedirs(baseline_save_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate the pretrained model on the test set (using test_file)\n",
    "baseline_results = model.evaluate(test_path, os.path.dirname(test_path), iou_threshold=0.4, savedir=baseline_save_dir)\n",
    "\n",
    "print(\"Baseline evaluation complete. Results saved to\", baseline_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline performance\")\n",
    "\n",
    "print(f\"Box Recall: {baseline_results['box_recall']:.4f}\")\n",
    "print(f\"Box Precision: {baseline_results['box_precision']:.4f}\")\n",
    "\n",
    "mean_iou = np.mean(baseline_results['results']['IoU'])\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test.csv file\n",
    "test_csv_path = os.path.join(extract_dir, \"test.csv\")\n",
    "df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Load the image\n",
    "image_name = \"20181001 (88).JPG\"\n",
    "image_path = os.path.join(extract_dir, image_name)\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image_np = np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter bounding boxes for the specific image\n",
    "image_boxes = df[df['image_path'] == image_name]\n",
    "\n",
    "# Prepare bounding boxes DataFrame for plot_predictions\n",
    "boxes_df = image_boxes[['xmin', 'xmax', 'ymin', 'ymax', 'label']]\n",
    "\n",
    "# Plot bounding boxes on the image\n",
    "annotated_image_truth = plot_predictions(image_np, boxes_df, color=(0, 165, 255), thickness=15)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "plt.imshow(annotated_image_truth)\n",
    "plt.axis('off')  # Turn off axis to focus on the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming baseline_results is already available\n",
    "predictions_df = baseline_results['predictions']\n",
    "\n",
    "# Filter predicted boxes for the specific image\n",
    "image_predictions = predictions_df[predictions_df['image_path'] == image_name]\n",
    "\n",
    "# Prepare bounding boxes DataFrame for plot_predictions\n",
    "# Use xmin, ymin, xmax, ymax and label columns from predictions\n",
    "boxes_df = image_predictions[['xmin', 'xmax', 'ymin', 'ymax', 'label']]\n",
    "\n",
    "# Plot bounding boxes on the image using deepforest.visualize\n",
    "annotated_image_base = plot_predictions(image_np, boxes_df, color=(0, 165, 255), thickness=15)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "plt.imshow(annotated_image_base)\n",
    "plt.axis('off')  # Turn off axis to focus on the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False negatives can be observed, where the baseline model failed to detect some cows present in the ground truth annotations. This highlights areas where the model's initial performance can be improved through fine-tuning using local data to reduce these missed detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Since the Binder environment does not provide enough computational resources for training the model, the following code demonstrates how to set up and train the model in a more powerful local or cloud-based environment using PyTorch Lightning.\n",
    "\n",
    "```python\n",
    "    import pytorch_lightning as pl\n",
    "\n",
    "    output_dir = os.path.join(notebook_folder, 'data')\n",
    "    test_dir = os.path.join(notebook_folder, 'test_data')\n",
    "\n",
    "    train_file = os.path.join(output_dir, \"train.csv\")\n",
    "    valid_file = os.path.join(output_dir, \"valid.csv\")\n",
    "    test_file = os.path.join(test_dir, \"test.csv\")\n",
    "\n",
    "    model.label_dict = {'cow': 0}  # Rename label\n",
    "\n",
    "    # Configure the model for GPU usage and set the CSV file paths\n",
    "    model.config['gpus'] = '-1'  # Use GPU (set to '0' for the first GPU or '-1' for all GPUs)\n",
    "    model.config[\"train\"][\"csv_file\"] = train_file  # Path to training CSV\n",
    "    model.config[\"train\"][\"root_dir\"] = os.path.dirname(train_file)  # Root directory for training images\n",
    "    model.config[\"score_thresh\"] = 0.4  # Set score threshold\n",
    "    model.config[\"train\"]['epochs'] = 5  # Number of epochs\n",
    "    model.config[\"validation\"][\"csv_file\"] = valid_file  # Path to validation CSV\n",
    "    model.config[\"validation\"][\"root_dir\"] = os.path.dirname(valid_file)  # Root directory for validation images\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='box_recall', patience=2, mode='max')\n",
    "\n",
    "    callback = ModelCheckpoint(\n",
    "        dirpath='checkpoints/',  # Directory to save checkpoints\n",
    "        monitor='box_recall',  # Metric to monitor\n",
    "        mode=\"max\",  # Save when the metric is maximized\n",
    "        save_top_k=3,  # Save the top 3 checkpoints\n",
    "        filename=\"box_recall-{epoch:02d}-{box_recall:.2f}\"  # File name format for checkpoints\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir='logdir/')\n",
    "\n",
    "    model.create_trainer(logger=TensorBoardLogger(save_dir='logdir/'),\n",
    "                                    callbacks=[callback, early_stopping])\n",
    "\n",
    "    model.trainer.fit(model)\n",
    "```\n",
    "\n",
    "After training the model, the following code saves the fine-tuned model checkpoint to a specified directory, ensuring that the model's progress can be easily reloaded or shared for further use. The checkpoint is saved in .ckpt format using PyTorch Lightning's save_checkpoint method.\n",
    "\n",
    "```python\n",
    "    checkpoint_dir = './drive/MyDrive/notebook/checkpoints_ckpt'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Save the model checkpoint as .ckpt\n",
    "    checkpoint_path = \"{}/finetuned_checkpoint.ckpt\".format(checkpoint_dir)\n",
    "\n",
    "    # Save the checkpoint after training using PyTorch Lightning's save_checkpoint method\n",
    "    model.trainer.save_checkpoint(checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the finetuned model checkpoint from Hugging Face\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"camappel/deepforest-livestock\",\n",
    "    filename=\"finetuned_checkpoint.ckpt\"\n",
    ")\n",
    "\n",
    "model = main.deepforest.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "print(\"Finetuned model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate finetuned performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate method assesses the fine-tuned model’s performance on the test dataset, with results stored for comparison against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label_dict = {'cow': 0}  # Assign a unique integer ID to the 'cow' label\n",
    "model.config['gpus'] = '-1'  # Use GPU (set to '0' for the first GPU or '-1' for all GPUs)\n",
    "model.config['workers'] = 0\n",
    "\n",
    "# Set the directory to save the results of the pretrained model\n",
    "finetuned_save_dir = os.path.join(notebook_folder, 'finetuned_pred_result')\n",
    "os.makedirs(finetuned_save_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate the pretrained model on the test set (using test_file)\n",
    "finetuned_results = model.evaluate(test_path, os.path.dirname(test_path), iou_threshold=0.4, savedir=finetuned_save_dir)\n",
    "\n",
    "print(\"Finetuned evaluation complete. Results saved to\", finetuned_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finetuned performance\")\n",
    "# Print box recall and precision with clean formatting\n",
    "print(f\"Box Recall: {finetuned_results['box_recall']:.4f}\")\n",
    "print(f\"Box Precision: {finetuned_results['box_precision']:.4f}\")\n",
    "\n",
    "# Compute and print the mean IoU, rounded to 4 decimal places\n",
    "mean_iou = np.mean(finetuned_results['results']['IoU'])\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = finetuned_results['predictions']\n",
    "\n",
    "image_predictions = predictions_df[predictions_df['image_path'] == image_name]\n",
    "\n",
    "# Use xmin, ymin, xmax, ymax and label columns from predictions\n",
    "boxes_df = image_predictions[['xmin', 'xmax', 'ymin', 'ymax', 'label']]\n",
    "\n",
    "annotated_image_fine = plot_predictions(image_np, boxes_df, color=(0, 165, 255), thickness=15)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # Create 1 row, 2 column layout\n",
    "\n",
    "axs[0].imshow(annotated_image_base)\n",
    "axs[0].set_title(\"Baseline Predictions\")\n",
    "axs[0].axis('off')  \n",
    "\n",
    "axs[1].imshow(annotated_image_fine)\n",
    "axs[1].set_title(\"Finetuned Predictions\")\n",
    "axs[1].axis('off') \n",
    "\n",
    "# Show the side-by-side plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model shows a significant improvement, successfully detecting all cows in the test imagery without any false negatives, demonstrating the effectiveness of fine-tuning on local data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of fine-tuning the DeepForest livestock detection model, achieving notable performance improvements after training for 5 epochs. The model was fine-tuned on a labeled aerial dataset of cows from Dataverse (2585 training and 808 validation samples). Performance on the test set, which consisted of images excluded from training and validation, showed substantial gains across key metrics:\n",
    "\n",
    "- Box Recall: Increased from 0.4405 to 0.9535\n",
    "- Box Precision: Improved from 0.5826 to 0.8587\n",
    "- Mean IoU: Enhanced from 0.3135 to 0.6571\n",
    "\n",
    "These results highlight the effectiveness of fine-tuning and open-sourcing, significantly boosting the model's ability to accurately detect livestock in aerial imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "**Dataset**: G.J. Franke; Sander Mucher, 2021, \"Annotated cows in aerial images for use in deep learning models\"\n",
    "\n",
    "**Codebase**: DeepForest [v1.4.0](https://github.com/weecology/DeepForest)\n",
    "\n",
    "**License**: The code in this notebook is licensed under the MIT License. The Environmental Data Science book is licensed under the Creative Commons by Attribution 4.0 license. See further details [here](https://github.com/alan-turing-institute/environmental-ds-book/blob/master/LICENSE.md).\n",
    "\n",
    "**Contact**: If you have any suggestion or report an issue with this notebook, feel free to [create an issue](https://github.com/alan-turing-institute/environmental-ds-book/issues/new/choose) or send a direct message to [environmental.ds.book@gmail.com](mailto:environmental.ds.book@gmail.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last tested: 2024-10-16\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "print(f'Last tested: {date.today()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
